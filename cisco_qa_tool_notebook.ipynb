{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1629a30-1399-434c-a231-92468edee41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLResponse\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae6c604-2567-4430-b13d-66d115b7066c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load_dotenv(find_dotenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../.env\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(\"../.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6b851-a11c-4f77-a42b-6ca42164bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.getenv('DB_PATH')\n",
    "proxy_name = os.getenv('PROXY_NAME')\n",
    "proxy_password = os.getenv('PROXY_PASSWORD')\n",
    "proxy_url = os.getenv('PROXY_URL')\n",
    "proxy_port = os.getenv('PROXY_PORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a462160-c7f4-4845-903c-11cfeb3fd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXIES = {\"http\":f\"http://{proxy_name}:{proxy_password}@{proxy_url}:{proxy_port}\",\n",
    "           \"https\":f\"http://{proxy_name}:{proxy_password}@{proxy_url}:{proxy_port}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84846465-3ed1-4d49-848a-bf3853675a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'www.cisco.com'\n",
    "additional_keywords = 'data sheet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd57fb6-d456-4a6b-9a16-d173e809e961",
   "metadata": {},
   "source": [
    "# get_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14330baa-6379-4b6e-b48b-cbc2161a503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code found from https://practicaldatascience.co.uk/data-science/how-to-scrape-google-search-results-using-python, with minor adjustments by myself.\n",
    "\n",
    "def get_source(url: str, PROXIES: dict = None) -> HTMLResponse:\n",
    "    \"\"\"Return the source code for the provided URL.\n",
    "    \n",
    "    Args:\n",
    "        url (string): URL of the page to scrape.\n",
    "        PROXIES (dict): dictionary of proxy IP addresses to use for HTTP requests\n",
    "        \n",
    "    Returns:\n",
    "        response (object): HTTP response object.\n",
    "    \"\"\"  \n",
    "    \n",
    "    from fake_useragent import UserAgent\n",
    "    \n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    \n",
    "    from requests_html import HTMLSession\n",
    "    \n",
    "    from urllib3.util.retry import Retry\n",
    "    \n",
    "    # Randomly generate headers to make HTTP get requests look like a user and not a bot\n",
    "    headers = {}\n",
    "    headers[\"User-Agent\"] = UserAgent().random\n",
    "    \n",
    "    # Initiate HTML session and set rules for retrying HTTP get requests upon failure\n",
    "    session = HTMLSession()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    # Initiate HTTP get request\n",
    "    response = session.get(url, headers=headers, proxies=PROXIES)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a791a7e5-8eae-4516-9643-08e3e6f28a2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'functions.web_tools.py'; 'functions.web_tools' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweb_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scrape_google\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'functions.web_tools.py'; 'functions.web_tools' is not a package"
     ]
    }
   ],
   "source": [
    "from functions.web_tools.py import scrape_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0137f-bc9e-4a56-a1ee-76ba0bb674fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_source(\"https://papir805.github.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032150c-7bb4-474b-ac6c-e13851333af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7bf5d0-fa3e-4162-b3d5-846f2275ee13",
   "metadata": {},
   "source": [
    "# scrape_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b9fb7-c89e-49f4-910d-50007c7d1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google(website_url: str, \n",
    "                  product_name: str, \n",
    "                  additional_search_keywords: str = 'none',\n",
    "                  language_code: str = 'en',\n",
    "                  PROXIES: dict = None,\n",
    "                  verbose: bool = False) -> list:\n",
    "    \"\"\"Return a list of URLs corresponding to a Google search focusing on a particular website.  Results from certain URLs that don't contain walkthroughs are excluded based on the URL's prefixes and suffixes.\n",
    "\n",
    "    Args:\n",
    "        website_url (string): URL for Google to focus its search on.  Example: www.yoururlhere.com\n",
    "        product_name (string): Name of the product to search for information on.\n",
    "        additional_search_keywords (string): Any additional text you want to include in the search to help refine the results.\n",
    "        language_code (boolean): Specifies which language to return search results in.\n",
    "        PROXIES (dict): proxy IP addresses to use for HTTP requests\n",
    "        verbose (boolean): Used for debugging\n",
    "\n",
    "    Returns:\n",
    "        links (list): A list of URLs corresponding to most recommended walkthroughs from www.gamefaqs.com for video_game_title on video_game_system.\n",
    "    \"\"\"\n",
    "    import urllib\n",
    "    \n",
    "    # Combine user input to construct the query used for the Google search\n",
    "    search_args = [website_url, product_name, additional_search_keywords]\n",
    "    joined_search_args = \" \".join(search_args)\n",
    "    query_suffix = urllib.parse.quote_plus(joined_search_args)  \n",
    "    query = f\"https://www.google.com/search?q=site%3A{query_suffix}&lr=lang_{language_code}\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(query_suffix)\n",
    "        print(query)\n",
    "    \n",
    "    # Initiate HTTP get request and scrape the URLs of the links corresponding to the search results\n",
    "    response = get_source(query, PROXIES)\n",
    "    links = list(response.html.absolute_links)\n",
    "    \n",
    "    # Establish criteria to remove search results corresponding to Google domains that contain information we don't care about\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.',\n",
    "                      'https://translate.google.')\n",
    "    suffixes = ()\n",
    "    \n",
    "    if verbose==True:\n",
    "        print(f\"{len(links)} links found initially.  Removing links that aren't useful.\")\n",
    "        print(links)\n",
    "    \n",
    "    # Remove search results based on the criteria established earlier\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "        if url.endswith(suffixes):\n",
    "            links.remove(url)\n",
    "    \n",
    "    # Sort remaining links\n",
    "    links.sort()\n",
    "    \n",
    "    st.write(f\"{len(links)} useful links found.\")\n",
    "    print(f\"{len(links)} useful links found.\")\n",
    "    \n",
    "    return links         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36a095-c840-476f-bb46-c86736de08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'catalyst 9400'\n",
    "\n",
    "links1 = scrape_google(website_url=website, \n",
    "                       product_name=product_name,\n",
    "                       additional_search_keywords=additional_keywords,\n",
    "                       verbose=True)\n",
    "links1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a3390-a021-45e2-9e4a-7079296d66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'catalyst 2500 wireless controller'\n",
    "\n",
    "links2 = scrape_google(website_url=website, \n",
    "                       product_name=product_name,\n",
    "                       additional_search_keywords=additional_keywords)\n",
    "links2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39006e16-b2c4-4c07-9bf1-754720d2f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'catalyst 2500 wireless controller'\n",
    "\n",
    "links2_1 = scrape_google(website_url=website, \n",
    "                       product_name=product_name,\n",
    "                       additional_search_keywords=additional_keywords)\n",
    "links2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cee0e-4141-426e-bdc2-83ecc40b809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'catalyst 4500e'\n",
    "\n",
    "links3 = scrape_google(website_url=website, \n",
    "                       product_name=product_name,\n",
    "                       additional_search_keywords=additional_keywords)\n",
    "links3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee32b74-edd3-4f0d-a2ee-ec4fe5b7dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'ir829'\n",
    "\n",
    "links4 = scrape_google(website_url=website, \n",
    "                       product_name=product_name,\n",
    "                       additional_search_keywords=additional_keywords)\n",
    "links4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cdc1a-0d80-4110-a1a8-803d86a95c9a",
   "metadata": {},
   "source": [
    "# load_pdf_as_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60481e9c-8604-4c78-9d0d-fc46d6cd46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_as_doc(html_path: str, local_path, filename: str) -> list[langchain.schema.document.Document]:\n",
    "    \"\"\"Loads PDF file from local_path and returns a list where each element is one page of the PDF file as a Langchain Document object.\n",
    "\n",
    "    Args:\n",
    "        html_path (string): URL where the PDF file was originally downloaded from\n",
    "        local_path (string): File path used to read the PDF file from local disk\n",
    "\n",
    "    Returns:\n",
    "        pdf_doc (list[langchain.schema.document.Document]): A list where each element is one page of the PDF file as a Langchain Document object\n",
    "    \"\"\"\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    \n",
    "    # Load local PDF file into memory \n",
    "    loader = PyPDFLoader(file_path=str(local_path))\n",
    "    \n",
    "    # Create a Langchain Document object for each page in the PDF file\n",
    "    pdf_doc = loader.load()\n",
    "    \n",
    "    # Add the URL where the PDF was downloaded from as metadata\n",
    "    for page in pdf_doc:\n",
    "        page.metadata['source'] = html_path\n",
    "        page.metadata['filename'] = filename\n",
    "        page.metadata['page'] = int(page.metadata['page']) + 1\n",
    "    \n",
    "    return pdf_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74551d-9945-4122-94ea-3fc99bf804c7",
   "metadata": {},
   "source": [
    "# download_pdf_and_return_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e02b4-1a10-4243-8d27-d5ea113c088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_and_return_doc(html_path: str, \n",
    "                                 pdf_filename: str, \n",
    "                                 proxies=None, \n",
    "                                 verbose=False):\n",
    "    \"\"\"\n",
    "    Initiates HTTP get request for the URL of a pdf file.  Writes the contents of the PDF file to local disk.\n",
    "\n",
    "    Args:\n",
    "        html_path (string): HTML URL of the PDF file\n",
    "        local_path (string): File path for saving the PDF file to local disk\n",
    "        proxies (dict): proxy IP addresses to use for HTTP requests\n",
    "        verbose (boolean): Used for debugging\n",
    "\n",
    "    Returns:\n",
    "        new_pages (list): A list of pdf pages after Cisco metadata has been added\n",
    "    \"\"\"\n",
    "    \n",
    "    import tempfile\n",
    "    import pathlib\n",
    "       \n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    temp_file_path = pathlib.Path(temp_dir.name) / pdf_filename\n",
    "    \n",
    "    # Get HTTP response of a URL\n",
    "    response = get_source(html_path, proxies)\n",
    "    \n",
    "    # Check to see if the HTTP response is actually for a PDF file\n",
    "    if response.headers['Content-Type'] == 'application/pdf':\n",
    "        # If the file is a PDF, write the contents to local_path            \n",
    "        with open(temp_file_path, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "            pdf_doc = load_pdf_as_doc(html_path, temp_file_path, pdf_filename)\n",
    "            \n",
    "            \n",
    "    \n",
    "        if verbose:\n",
    "            print(f\"{temp_file_path} written to disk\")\n",
    "            \n",
    "        return pdf_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e39e2f-a5d5-4174-a8e8-595a33f35537",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_response = get_source(\"https://www.cisco.com/en/US/prod/collateral/routers/ps12558/ps12559/datasheet-c78-730862.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6482db-b9e3-4b54-a371-85569b23f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_response.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5b5c2-85c7-416c-b4a3-129f7c3ffccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_doc = download_pdf_and_return_doc(\"https://www.cisco.com/en/US/prod/collateral/routers/ps12558/ps12559/datasheet-c78-730862.pdf\", f\"datasheet-c78-730862.pdf\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115740fe-9a5d-4158-b522-b8e6f6938e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_cisco_metadata(pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64119a7-d5f2-4a09-9895-26f2eee7ea4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08c443-792a-4674-8f84-daa2de9ebe1f",
   "metadata": {},
   "source": [
    "# generate_cisco_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a7c26-6c09-4fac-ae56-5d6164c0f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cisco_metadata(pdf_doc: list, verbose: bool = False) -> list:\n",
    "    \"\"\"Extracts information from the URL of a data sheet pulled from www.cisco.com to be used as metadata in the document.\n",
    "\n",
    "    Args:\n",
    "        pdf_doc (list): A list where each element is a single page of a pdf file\n",
    "        verbose (boolean): Used for debugging\n",
    "\n",
    "    Returns:\n",
    "        new_pages (list): A list of pdf pages after Cisco metadata has been added\n",
    "    \"\"\"\n",
    "    \n",
    "    new_pages = []\n",
    "    failed_docs = []\n",
    "    failed = False\n",
    "    \n",
    "    for page in pdf_doc:  \n",
    "    # Many source URLs for Cisco datasheets follow this structure:\n",
    "    # https://www.cisco.com/.../collateral/product_category/product_name/filename.pdf\n",
    "\n",
    "    # Separates the URL into a list of the form:\n",
    "    # ['...', 'www.cisco.com', '...', 'collateral', 'product_category', 'product_name', 'filename']\n",
    "        pdf_html_path = page.metadata['source']\n",
    "        split_url = pdf_html_path.split('/')\n",
    "\n",
    "        try:\n",
    "            # Find the index of \"collateral\" in the split_url and set metadata based on that index\n",
    "            idx = split_url.index('collateral')\n",
    "\n",
    "            product_category = split_url[idx+1]\n",
    "            product_name = split_url[idx+2]\n",
    "\n",
    "            page.metadata[\"product_category\"] = product_category\n",
    "            page.metadata[\"product_name\"] = product_name\n",
    "\n",
    "        except ValueError as e:\n",
    "            # If \"collateral\" isn't present in the split_url, don't generate any new metadata\n",
    "            failed = True\n",
    "            print(e)\n",
    "            pass\n",
    "        \n",
    "        new_pages.append(page)\n",
    "            \n",
    "        if verbose:\n",
    "            if failed:\n",
    "                print(f\"Failed to get metadata for {pdf_html_path}\")\n",
    "            else:\n",
    "                print(f\"\"\"Product category: {pdf_doc[0].metadata[\"product_category\"]}, \\nProduct_name: {pdf_doc[0].metadata[\"product_name\"]}, \\nMetadata added for {pdf_doc[0].metadata[\"filename\"]}.\"\"\")\n",
    "    \n",
    "    return new_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6800dd-81ba-4835-bb92-53dd254c724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = ['a', 'collateral', 'b']\n",
    "test_list.index('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea3205-51f9-4df4-8b42-85afa8962a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = generate_cisco_metadata(pdf_doc, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7700dd5-5c7b-4a66-b056-f8563db68cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce568df1-823d-4b9a-8f86-a02b96b029e9",
   "metadata": {},
   "source": [
    "# return_pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc75022-5496-4561-9384-1fe34782009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pdf_docs(links: list, \n",
    "                    verbose: bool = False, \n",
    "                    proxies: dict = None,\n",
    "                    is_cisco_datasheet: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Download PDFs from URLs, load from local disk as Langchain Documents and add metadata.  Filters URLs so that duplicate PDF files are ignored.\n",
    "\n",
    "    Args:\n",
    "        links (list): A list of URLs\n",
    "        verbose (boolean): Used for debugging\n",
    "        proxies (dict): proxy IP addresses to use for HTTP requests\n",
    "        is_cisco_datasheet (boolean): Extracts information to be used as metadata based on criteria specifically designed for Cisco data sheets\n",
    "\n",
    "    Returns:\n",
    "        unique_pdf_docs (list): A list of Langchain PDF documents\n",
    "    \"\"\"\n",
    "\n",
    "    unique_pdf_docs = []\n",
    "    unique_pdf_names = []\n",
    "\n",
    "    for link in links:\n",
    "        # Cisco datasheets are available as both a HTML file and a PDF file.  Any datasheet that has a URL ending with .html can be accessed from the same URL if the .html is replaced with .pdf.  \n",
    "        if link.endswith(\".pdf\"):\n",
    "            pdf_html_path = link\n",
    "        else:\n",
    "            pdf_html_path = link.split(\".html\", 1)[0] + \".pdf\"\n",
    "        \n",
    "        pdf_name = pdf_html_path.split(\"/\")[-1]\n",
    "        #pdf_local_path = f\"./data/{pdf_name}\"\n",
    "        \n",
    "        if pdf_name not in unique_pdf_names:\n",
    "            # Download PDFs only if they haven't already been downloaded\n",
    "            unique_pdf_names.append(pdf_name)\n",
    "\n",
    "            try:\n",
    "                # Attempt to load the PDF file from a local path and return the PDF as a Langchain Document object\n",
    "                pdf_doc = download_pdf_and_return_doc(html_path=pdf_html_path,\n",
    "                                                     pdf_filename=pdf_name,\n",
    "                                                     proxies=proxies)\n",
    "\n",
    "                if is_cisco_datasheet==True:\n",
    "                    # Add metadata if PDF is a Cisco datasheet\n",
    "                    pdf_doc = generate_cisco_metadata(pdf_doc)\n",
    "                    unique_pdf_docs.append(pdf_doc)\n",
    "                else:\n",
    "                    unique_pdf_docs.append(pdf_doc)\n",
    "\n",
    "                print(f\"File {pdf_name} downloaded and successfully loaded.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {link} not available as PDF file.\")\n",
    "                if verbose:\n",
    "                    print(e)\n",
    "                    print(f\"Error occurred: {link} not available as PDF file.\")\n",
    "            #     # Cleanup step removing PDF file after being loaded as it isn't needed anymore\n",
    "            # if os.path.exists(pdf_local_path):\n",
    "            #     os.remove(pdf_local_path)\n",
    "\n",
    "    return unique_pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236d378-806e-43df-ad29-adba4f9bd109",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs1 = return_pdf_docs(links1, is_cisco_datasheet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e44a0-9961-41ec-baee-2917a4fd9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in pdf_docs1:\n",
    "    print(doc[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a215e33-5451-451e-a9d6-7b29a83f784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs2 = return_pdf_docs(links2, is_cisco_datasheet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d992bfb-441d-4c80-9b2b-b883fb6af607",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in pdf_docs2:\n",
    "    print(doc[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88f158-8484-42d7-8c13-814c58563a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs3 = return_pdf_docs(links3, is_cisco_datasheet=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f78278-80df-4cc8-9cc5-56b6b6fc5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in pdf_docs3:\n",
    "    print(doc[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02273d9-242e-40a6-afb0-24a90ce17e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs4 = return_pdf_docs(links4, is_cisco_datasheet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af886b-1018-4d3c-a594-698f2750ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in pdf_docs4:\n",
    "    print(doc[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d970b8-0a05-4695-a54a-2e199496cf69",
   "metadata": {},
   "source": [
    "# return_pinecone_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a2baf-c462-4a55-ac08-47bddccc6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pinecone_vectorstore(index_name: str, \n",
    "                                model_name: str = 'text-embedding-ada-002'\n",
    "                               ) -> langchain.vectorstores.pinecone.Pinecone:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    import pinecone\n",
    "    import os\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    \n",
    "    # intialize pinecone\n",
    "    pinecone.init(\n",
    "        api_key=os.getenv(\"PINECONE_API_KEY\"), # find at app.pinecone.io\n",
    "        environment=os.getenv(\"PINECONE_ENV\"), # next to api key in console\n",
    "    )\n",
    "\n",
    "    # First, check if our index already exists.  If it doesn't we create it\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        # we create a new index\n",
    "        pinecone.create_index(\n",
    "            name=index_name,\n",
    "            metric='cosine',\n",
    "            dimension=1536\n",
    "        )\n",
    "        \n",
    "    # Specify which vector embeddings to use\n",
    "    embedding = OpenAIEmbeddings(model=model_name)\n",
    "    \n",
    "    # Create the Langchain vector database object\n",
    "    index = pinecone.Index(index_name)\n",
    "    vectordb = Pinecone(index, embedding, 'text')\n",
    "    \n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b953c-355d-4896-85bc-3d9edfa8c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"cisco-knowledgebase\"\n",
    "vectordb_pinecone = return_pinecone_vectorstore(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec79276-c553-42cf-b9fa-d0f7cdc28a95",
   "metadata": {},
   "source": [
    "# return_chroma_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1719c1-9974-425e-871b-8521f53e7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_chroma_vectorstore(persist_directory: str) -> langchain.vectorstores.chroma.Chroma:\n",
    "    \"\"\"\n",
    "    If a persist_directory is provided, attemps to return the vector store located there.  If a persist_directory is not provided, one is created called 'db' by default.\n",
    "    \n",
    "    Returns:\n",
    "        vectordb (langchain.vectorstores.chroma.Chroma): A langchain object that is used to access the vectorstore called 'db' that was created.\n",
    "    \"\"\"\n",
    "    \n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    \n",
    "    # Specify which vector embeddings to use\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    \n",
    "    if persist_directory:\n",
    "        # Get vectorstore from persist_directory on local disk\n",
    "        print(f'getting vectorstore named {persist_directory}')\n",
    "        vectordb = Chroma(persist_directory=persist_directory,\n",
    "                      embedding_function=embedding)\n",
    "        print('got vectorstore')\n",
    "    else:\n",
    "        # Create vectorstore at ./db on local disk\n",
    "        print(f\"persist directory not given \\ncreating vectorstore called db\")\n",
    "        vectordb = Chroma(persist_directory='db',\n",
    "                      embedding_function=embedding)\n",
    "        print('vectorstore created')       \n",
    "        vectordb.persist()\n",
    "    \n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8dfc02-2cc3-465a-ba5e-8b6a1630a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb_chroma = return_chroma_vectorstore(f\"{db_path}/cisco_db/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135482c-371c-4809-adb2-fa1be3fa2278",
   "metadata": {},
   "source": [
    "# return_new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ae889-3734-4aa3-8bd1-0a526d317c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_url = \"https://www.cisco.com/c/dam/global/en_sg/training-events/datacentertechbyte/assets/pdfs/n1000_datasheet.pdf\"\n",
    "\n",
    "web_url.split(\".html\", 1)[0] + \".pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104f55a-08a8-4685-a692-fb3eefd2b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_new_links(links: list, vectordb, verbose: bool = False) -> list:\n",
    "    \"\"\"Checks an existing vectordb whether it contains any documents having URLs matching those in links.  Any URLs found to match are removed as they imply the document already exists in the vectordb and doesn't need to be added again.\n",
    "\n",
    "    Args:\n",
    "        links (list): a list of URLs\n",
    "        vectordb (): a vectordatabase \n",
    "        debug (bool): prints when a URL doesn't match any that already exist in the vectordb.\n",
    "    \n",
    "    Returns:\n",
    "        new_links (list): a list of URLs not already present in vectordb.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_links = []\n",
    "\n",
    "    for url in links:\n",
    "        if url.endswith('.pdf'):\n",
    "            pdf_html_path = url\n",
    "        # Find which urls in links match with PDF files already in the vectordb\n",
    "        else:\n",
    "            pdf_html_path = url.split(\".html\", 1)[0] + \".pdf\"\n",
    "        \n",
    "        matches = vectordb.similarity_search(query=' ', \n",
    "                                             k=1, \n",
    "                                             filter={'source':pdf_html_path})\n",
    "        # If any matches are found, it implies the PDF file exists in the vectordb and we can ignore the url\n",
    "        if len(matches)>0:\n",
    "            continue\n",
    "        \n",
    "         # If no matches are found, keep the URL so the PDF file can be added to vectordb later\n",
    "        else:\n",
    "            if verbose==True:\n",
    "                print(f'{url} not found in database')\n",
    "            new_links.append(url)\n",
    "    \n",
    "    return new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4a186-8f40-487c-9728-eb0a0b53ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links = return_new_links(links1, vectordb_pinecone, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad7aba-0929-4cb3-8ae1-198304156992",
   "metadata": {},
   "source": [
    "# return_new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d132551-603b-43d6-a400-2826cece339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_new_docs(pdf_docs: list, \n",
    "                    vectordb, \n",
    "                    verbose: bool = False) -> list:\n",
    "    \"\"\"Checks an existing vectordb whether it contains any documents having a filename matching one already in the vectordb.  Any documents found to match are ignored as they imply the document already exists in the vectordb and doesn't need to be added again.\n",
    "\n",
    "    Args:\n",
    "        docs (list): a list of LangChain Document objects\n",
    "        vectordb (): a vectordatabase\n",
    "        debug (bool): prints when a document for a particular video_game_title and walkthrough_id doesn't match any already present in the vectordb\n",
    "    \n",
    "    Returns:\n",
    "        new_docs (list): a list of LangChain Document objects not already present in vectordb.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_docs = []\n",
    "    \n",
    "    # Check if any pdf_docs have a filename that already exist in vectordb.\n",
    "    for doc in pdf_docs:\n",
    "        filename = doc[0].metadata['filename']\n",
    "        matches =  vectordb.similarity_search(query=' ',k=1,filter=\n",
    "                                        {\n",
    "                                            \"filename\": {\"$eq\": filename}\n",
    "                                        }\n",
    "                                             )\n",
    "        # If any matches are present, the doc already exists in the vectordb and can be ignored \n",
    "        if len(matches) > 0:\n",
    "            pass\n",
    "        \n",
    "        # If no matches are found, keep the doc so it can be added to vectordb later\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'{filename} not found in database')\n",
    "            new_docs.append(doc)\n",
    "    print(f\"{len(new_docs)} new document(s) not found in database.\")\n",
    "    \n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205bdcc-7e6a-4e56-bab2-655c9e1570bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs1 = return_new_docs(pdf_docs1, vectordb_pinecone, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d7613-0350-4484-85fa-7f807d7f224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs2 = return_new_docs(pdf_docs2, vectordb_pinecone, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004de9-553c-4a9d-8570-d7ee25aa98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs3 = return_new_docs(pdf_docs3, vectordb_pinecone, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39314c-544b-45a9-8239-1a5fb731b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs4 = return_new_docs(pdf_docs4, vectordb_pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0176e6-8995-4e75-b2ce-257b77387578",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs4[0][0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635eeec-ba58-4960-b024-48a3f9f0c6bc",
   "metadata": {},
   "source": [
    "# token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f1e4b-a767-434b-b61d-0defb25291e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_counter(text):\n",
    "    import tiktoken\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "    \n",
    "    tokens = tokenizer.encode(text,\n",
    "                              disallowed_special=()\n",
    "                             )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5830d79-78b9-4033-ac73-ecd98653f440",
   "metadata": {},
   "source": [
    "# batch_and_add_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401296b-8d73-498d-ad66-1424cbcc1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_and_add_texts(texts: list, vectordb) -> None:\n",
    "    \"\"\"\n",
    "    OpenAI enforces a token/min limit for embedding tokens. This function avoids hitting that limit by splitings texts into batches less than or equal to the token_limit.  After each batch embedded and added to the vectorstore, the program waits 60 seconds to avoid hitting the limit.\n",
    "\n",
    "    Args:\n",
    "        texts (list): a list of chunked LangChain Document objects\n",
    "        vectordb (): a vectordatabase\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Set max_tokens to be 10_000 less than the limit OpenAI enforces\n",
    "    max_tokens = 1_000_000 - 10_000\n",
    "    \n",
    "    # Flatten a list of lists into a single list containing each page of all PDF docs in texts\n",
    "    flattened_pages = [page for pdf_doc in texts for page in pdf_doc]\n",
    "    \n",
    "    # Total number of pages across all PDF docs\n",
    "    num_pages = len(flattened_pages)\n",
    "    print(f'num_pages: {num_pages}')\n",
    "    \n",
    "    total_tokens = 0\n",
    "    batched_pages = []\n",
    "        \n",
    "    for _ in range(num_pages):\n",
    "        # Remove page from list, count how many tokens it has and add the count to total_tokens\n",
    "        current_page = flattened_pages.pop(0)\n",
    "        batched_pages.append(current_page)\n",
    "        num_tokens = token_counter(current_page.page_content)\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "        # If total_tokens is less than the limit, continue the loop and add more pages of the PDF file to the batch\n",
    "        if total_tokens <= max_tokens:\n",
    "            continue\n",
    "        \n",
    "        # If the max_tokens limit is exceeded, insert the batched pages into the vectordb.\n",
    "        else:\n",
    "            pages_inserted=len(batched_pages)\n",
    "            remaining_pages=len(flattened_pages)\n",
    "            print(f\"Inserted {pages_inserted} pages into database.  {remaining_pages} pages remaining.\")\n",
    "            vectordb.add_documents(batched_pages)\n",
    "            # Reset the total_tokens count and which pages are in the batch so they're ready for the rest of the loop.  \n",
    "            total_tokens = 0\n",
    "            batched_pages = []\n",
    "            \n",
    "            # Sleep for 60 seconds to make sure the limit isn't hit on the next batch\n",
    "            print('sleeping for 60 seconds')\n",
    "            time.sleep(60)\n",
    "    \n",
    "    # If the last iteration of the for loop occurs and the if condition is true, then the batched_texts won't get uploaded.  This checks to see if any batched texts are present after the loop is complete and adds them. \n",
    "    if len(batched_pages)>0:\n",
    "        pages_inserted=len(batched_pages)\n",
    "        print(f\"Inserted {pages_inserted} pages into database.  Process completed.\")\n",
    "        vectordb.add_documents(batched_pages)\n",
    "        \n",
    "    #vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d676cc1-fcf2-48ac-9990-12c568bbb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_and_add_texts(pdf_docs1, vectordb_pinecone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b2ae7-7218-4d9c-b045-813c532d9860",
   "metadata": {},
   "source": [
    "# get_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bd586-bacf-4ca1-8de2-c2115de9b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(vectordb, \n",
    "                     product_question: str,\n",
    "                     top_n_chunks: int, \n",
    "                     model: str) -> dict:\n",
    "    \"\"\"Returns a dictionary containing information about a query to a LLM.  The dictionary contains the product_question itself, the LLM's response to the product_question, and the chunks that were pulled from vectordb to answer the question.\n",
    "\n",
    "    Args:\n",
    "        vectordb (): a vectordatabase containing source documents used to answer product_question\n",
    "        top_n_chunks (int): How many chunks are retrieved from the vector store to be used in answering the product_question\n",
    "        model (str): Name of the LLM model to use\n",
    "        product_question (str): A question you want answered about a video game\n",
    "        video_game_titles (list): A list of titles corresponding to the video game being asked about.  Many video games on GameFAQs have different names corresponding to the same game.  This list will enable the RetrievalQA chain to focus attention on only documents that correspond to the game being asked about.\n",
    "    \n",
    "    Returns:\n",
    "        llm_response (dict): \n",
    "    \"\"\"\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    \n",
    "    search_kwargs = {'k': top_n_chunks}\n",
    "    \n",
    "    retriever = vectordb.as_retriever(search_kwargs=search_kwargs)\n",
    "    \n",
    "\n",
    "    \n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question.  If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    PROMPT = PromptTemplate(template=prompt_template, \n",
    "                            input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "    llm = ChatOpenAI(model_name=model)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "                                    llm=llm, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=retriever, \n",
    "                                    return_source_documents=True,\n",
    "                                    chain_type_kwargs=chain_type_kwargs\n",
    "                                    )\n",
    "    \n",
    "    llm_response = qa_chain(product_question)\n",
    "    \n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337c0f1-ceeb-4203-8f66-c9900669116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_response = get_llm_response(vectordb_pinecone, query1, 10, model='gpt-3.5-turbo-16k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c27e92-92a9-4c21-a9f8-4583242023bc",
   "metadata": {},
   "source": [
    "# process_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f5dc8-9a8e-4ef0-b051-f6bd1f57caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response: dict, \n",
    "                         print_sources: bool = False, \n",
    "                         print_chunks: bool = False) -> None:\n",
    "    \"\"\"Access and print the answer to a llm query.  Additionally print the URL of any source document used to answer the question, as well as which chunks from that source document were used.\n",
    "\n",
    "    Args:\n",
    "        llm_response (dict): A dictionary containing the response to a llm query and which source documents and chunks were used.\n",
    "        print_sources (bool): Specifies whether to print the source documents used.\n",
    "        print_chunks (bool): Specifies whether to print the chunks that were used.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(llm_response['result'])\n",
    "    if print_sources:\n",
    "        print('\\n\\nSources:')\n",
    "        \n",
    "        unique_sources = []\n",
    "\n",
    "        for source in llm_response['source_documents']:\n",
    "            source_url = source.metadata['source']\n",
    "            source_page = source.metadata['page']\n",
    "            sources = (source_url, source_page)\n",
    "            if sources not in unique_sources:\n",
    "                unique_sources.append(sources)\n",
    "        \n",
    "        for i, source in enumerate(unique_sources, 1):\n",
    "            source_url = source[0]\n",
    "            source_page_num = int(source[1])\n",
    "            print(f\"{i}. {source_url}#page={source_page_num} - Page {source_page_num}\")\n",
    "    print()\n",
    "    if print_chunks:\n",
    "        print('\\n\\nChunks:')\n",
    "        for i, chunk in enumerate(llm_response[\"source_documents\"], 1):\n",
    "            print(f'----------Chunk {i}----------')\n",
    "            print(chunk.page_content)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e3b60-81d6-4673-9294-64518ff1aab5",
   "metadata": {},
   "source": [
    "# cisco_qa_search_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801f391-7066-4e50-8cb4-0b4309abc2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cisco_qa_search_tool(product_question: str, \n",
    "                         product_name: str,\n",
    "                         index_name: str = \"cisco-knowledgebase\", \n",
    "                         top_n_chunks: int = 5, \n",
    "                         print_sources: bool = False, \n",
    "                         print_chunks: bool = False, \n",
    "                         model:str = 'gpt-3.5-turbo',\n",
    "                         proxies: dict = None,\n",
    "                         verbose: bool = False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    links = scrape_google(website_url=\"www.cisco.com\",\n",
    "                          product_name=product_name, \n",
    "                          additional_search_keywords=\"data sheets\",\n",
    "                          PROXIES=proxies,\n",
    "                          verbose=verbose)\n",
    "    \n",
    "    vectordb = return_pinecone_vectorstore(index_name)\n",
    "    \n",
    "    new_links = return_new_links(links, \n",
    "                                 vectordb, \n",
    "                                 verbose=verbose)\n",
    "\n",
    "    num_new_links = len(new_links)\n",
    "    \n",
    "    if num_new_links > 0:\n",
    "        print(f'{num_new_links} new URLs detected')\n",
    "        \n",
    "        if verbose:\n",
    "            print(new_links)\n",
    "        \n",
    "        docs = return_pdf_docs(new_links, \n",
    "                               verbose=verbose, \n",
    "                               proxies=proxies,\n",
    "                               is_cisco_datasheet=True)\n",
    "        \n",
    "        new_docs = return_new_docs(docs, \n",
    "                                   vectordb, \n",
    "                                   verbose=verbose)\n",
    "        \n",
    "        if len(new_docs) > 0:\n",
    "            batch_and_add_texts(new_docs, vectordb)\n",
    "            \n",
    "        else:\n",
    "            print(\"No new docs to add\")\n",
    "    else:\n",
    "        print(\"No new links to add\")\n",
    "    \n",
    "    question_with_product_name = product_question + ' ' + product_name\n",
    "    \n",
    "    llm_response = get_llm_response(vectordb, \n",
    "                                    question_with_product_name,\n",
    "                                    top_n_chunks, \n",
    "                                    model)\n",
    "    \n",
    "    processed_response = process_llm_response(llm_response, \n",
    "                                              print_sources,\n",
    "                                              print_chunks)\n",
    "    \n",
    "    return processed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd04a24-644a-4a97-90d0-c0bef9028762",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    print('a')\n",
    "else:\n",
    "    print('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99092f28-235c-4c2e-845d-bb777c6cf86b",
   "metadata": {},
   "source": [
    "# catalyst 2500 wireless controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d7065-1cec-4254-b22f-301634142edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'catalyst 2500 wireless controller'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c45b40-d305-4f32-95ff-18225ef21aee",
   "metadata": {},
   "source": [
    "## query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4014363-bd44-4bbe-af80-9da1d69b62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = 'how many access points do the cisco 2500 series wireless controllers have?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d1fc0-0ed3-4864-9c94-5fbc373049f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cisco_qa_search_tool(product_question=query1,\n",
    "                     product_name=product_name,\n",
    "                     top_n_chunks = 5, \n",
    "                     print_sources=True,\n",
    "                     print_chunks=True, \n",
    "                     model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e2cbe-7ca1-4963-a116-715acb0d7e21",
   "metadata": {},
   "source": [
    "# catalyst 9400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9175c-f1cd-4d5c-b89d-8cb6baf346ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'catalyst 9400'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd8442-ec1b-4949-a145-df861ce5fe39",
   "metadata": {},
   "source": [
    "## query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f778b-e42c-4075-b2e1-cd40f41c5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What ranges of temperatures can it endure?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4142e25-ebc2-4b74-bbeb-a695d1a72a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cisco_qa_search_tool(product_question=query1,\n",
    "                     product_name=product_name,\n",
    "                     top_n_chunks = 5, \n",
    "                     print_sources=True,\n",
    "                     print_chunks=True, \n",
    "                     model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3728e0-2054-43f0-990e-ae7854eed3c0",
   "metadata": {},
   "source": [
    "# catalyst 4500e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89915515-d910-4b56-a673-9ce8602bf589",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'catalyst 4500e'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27046d81-540c-426d-b668-265043f6c536",
   "metadata": {},
   "source": [
    "## query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3466f-7984-43c0-bb30-4f7d3019f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What throughput does it have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551af64-a971-4cf1-90ef-b10f08f8e33a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cisco_qa_search_tool(product_question=query1,\n",
    "                     product_name=product_name,\n",
    "                     top_n_chunks = 10, \n",
    "                     print_sources=True,\n",
    "                     print_chunks=True, \n",
    "                     model='gpt-3.5-turbo-16k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc74c3-e522-4f73-87f1-ec0ea7ae3ada",
   "metadata": {},
   "source": [
    "# network analysis module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4973104-415c-455e-bc2e-2b6625d84076",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'network analysis module'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1592e-aa7b-42c7-98b4-55b0616fba93",
   "metadata": {},
   "source": [
    "## query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23405115-251d-4035-9b39-ff1e741d2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"what are the features of a network analysis module\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7198fc-4d6f-415f-ba42-dfb5cb185048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cisco_qa_search_tool(product_question=query1,\n",
    "                     product_name=product_name,\n",
    "                     top_n_chunks = 10, \n",
    "                     print_sources=True,\n",
    "                     print_chunks=True, \n",
    "                     model='gpt-3.5-turbo-16k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ca76e-784e-4ce3-b90b-64993b70ed1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
